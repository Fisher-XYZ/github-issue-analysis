index,repoName,issueNumber,commentIndex,category,commentBody
760,elasticsearch,12683,0,[RC/PA],mention thank you for reporting this issue the underlying issue is that because of the analyzer you have configured the stop analyzer the query is parsed to the null query this issue helped us uncover a deeper bug in our handling of named queries that are null we will have a fix in place soon 
761,elasticsearch,13773,0,RC,this issue was happening in beta and does not have been reported since then closing 
762,elasticsearch,14223,0,PA,this inconsistency comes from x where the terms query would generate a bool query and the terms filter would generate a terms filter now that we merged both we have to accept the code parameter in all cases and i think the bug is that we don't fail when code is set while we are in a code context code is currently deprecated so that we can make the code query always generate a lucene termsquery instead of a booleanquery when used in a query context which i think is important for the predictability of the query dsl otherwise queries would have very different performance characteristics when used as a filter or a query would fail when there are more than clauses when used as a query and not a filter etc this will help remove the inconsistency as well when code is finally removed i agree the bool syntax is a bit verbose but i don't think that the terms query should be the solution maybe we can find ways to simplify the bool syntax when all sub queries are term queries
763,elasticsearch,14223,1,SD,min should match hasn't been documented in the terms query since let's deprecate it now and remove it in
764,elasticsearch,14223,2,SE,closing as minimum should match has been removed from terms query in: urlothers
765,elasticsearch,14453,0,SD,the value of home used when creating the elasticsearch user should probably be set to something other than home elasticsearch that exists and has executable permissions for the elasticsearch user in the interim you can just set home to something else like code to get your cron jobs running again 
766,elasticsearch,14453,1,SE,i just have the puppet role create home elasticseach its a packaging bug creating a user with the wrong home directory set either create the home directory or like you said set home to var lib elasticsearch or whatever 
767,elasticsearch,14453,2,SE,on it doesn't look like we're setting the home directory no urlothers is this user not a hangover from a previous installation
768,elasticsearch,14453,3,SE,if we don't set the home directory i think it defaults to whatever the base home directory is in etc default useradd plus the username so usually this ends up being code even for system users
769,elasticsearch,14453,4,PA,mention i don't think that the file has changed since at least urlothers before that it was using the specific scripts: urlothers i went ahead and installed es and es in reverse order onto a centos vm via code none of them created a code entry i cannot find where we may have created it at some point perhaps we never did
770,elasticsearch,14453,5,SD,reference in es an effort has been made to unify the behaviour of package install scripts among the different linux distributions and i think the change has been made at this time elasticsearch does not need any home directory to work so it is not created by the scripts depending of the distribution code does a bunch of things and among them it tries to change directory to user's home which does not exist you're not forced to code in order to run a cron job
771,elasticsearch,14453,6,[SE/PA],i don\'t know what the original version of es was installed on that cluster but somewhere along the line home elasticsearch was created it was a cron job failing on a redeploy that kicked out the message if it\'s not expected to create it then i guess it\'s not a bug if i look at most other system type users in rhel centos they normally set the homedir to in the passwd file or in the case of some services that store a lot of data their var dir debian seems to just create a home user dir for non data storing services tldr seems weird to create a user with a home directory that doesn\'t exist i\'d vote either create the dir to be consistent with debian or point it to to be consistent rhel 
772,elasticsearch,14453,7,PC, home elasticsearch is also useful to house java policy files as discussed here: urlothers
773,elasticsearch,14453,8,SD,just extend the code command by code please 
774,elasticsearch,14453,9,RC,guys are you planning first anniversary celebration because it is almost year old i have checked latest version rpm and it is still making mess in user configuration is this one liner fix so time consuming the bug which was closed as duplicate was actually better titled because the problem is not that home directory is not being created but because the correct one is undefined during user creation in rpm jakefromthedark solution is correct the other is similar but with var lib elasticsearch set as home dir pretty please with a sugar on top fix it :
775,elasticsearch,14453,10,PA,the issue is still prevalent in elasticsearch x where the code user is created with a non existing home directory code confirmed with package code on centos please re open this issue 
776,elasticsearch,14453,11,PC,mention it was an intentional decision to explicitly not create the home dir as code is a system user please don't run cron with the code user
777,elasticsearch,14453,12,[PC/PR],although i'm not sure setting an invalid home directory is the right way to prevent nudge users not to run cron with the code user i do understand now why you wouldn't want that happening thank you for your reply i will discontinue using the code user for our code cron jobs
778,elasticsearch,14573,0,PC,you're specifying the custom config file location incorrectly see urlothers
779,elasticsearch,14573,1,RC,hi mention thanks for quick response as you can see from the description i've provided i was using option ddefault path conf i tried again same command with option path conf there was no exception because of config access issue but i had to specify also path data and path logs because for some reason those settings were ignored in the config i've provided in my config i also specify nonstandard ports to use and those settings are also not used any advise what can be wrong thanks kirill
780,elasticsearch,14573,2,RO,looks like config is ignored completely if i specify all options via command line i still get exception like above: pre sudo u elasticsearch usr share elasticsearch bin elasticsearch path conf etc tribe elasticsearch path logs var log elasticsearch path data var lib elasticsearch transport tcp port urlothers network host tribels cluster name logstash data tribe els discovery zen ping multicast enabled false tribe els discovery zen ping unicast hosts log j:warn no appenders could be found for logger bootstrap log j:warn please initialize the log j system properly log j:warn see urlothers for more info exception in thread main java security accesscontrolexception: access denied java io filepermission usr share elasticsearch config elasticsearch yml read at java security accesscontrolcontext checkpermissionaccesscontrolcontext java: at java security accesscontroller checkpermissionaccesscontroller java: at java lang securitymanager checkpermissionsecuritymanager java: at java lang securitymanager checkreadsecuritymanager java: at sun nio fs unixpath checkreadunixpath java: at sun nio fs unixfilesystemprovider checkaccessunixfilesystemprovider java: at java nio file files existsfiles java: at org elasticsearch node internal internalsettingspreparer prepareenvironmentinternalsettingspreparer java: at org elasticsearch node node init node java: at org elasticsearch node nodebuilder buildnodebuilder java: at org elasticsearch tribe tribeservice init tribeservice java: at sun reflect nativeconstructoraccessorimpl newinstance native method at sun reflect nativeconstructoraccessorimpl newinstancenativeconstructoraccessorimpl java: at sun reflect delegatingconstructoraccessorimpl newinstancedelegatingconstructoraccessorimpl java: at java lang reflect constructor newinstanceconstructor java: at guice at org elasticsearch node node init node java: at org elasticsearch node nodebuilder buildnodebuilder java: at org elasticsearch bootstrap bootstrap setupbootstrap java: at org elasticsearch bootstrap bootstrap initbootstrap java: at org elasticsearch bootstrap elasticsearch mainelasticsearch java: pre 
781,elasticsearch,14573,3,RC,thanks for persisting i've managed to replicate this and it is indeed a bug when the tribe node attempts to instantiate a node for the tribe service it checks for access to the config directory but that setting is no longer available to it and so it defaults to checking for path home this can be replicated with a simple config file saved as code : codearea start elasticsearch as: codearea and it fails with: codearea
782,elasticsearch,14573,4,NT,mention could you take a look at this please 
783,elasticsearch,14573,5,[PA/SD],i had a look at this only selected settings are forwarded to the inner tribe clients from the tribe node code is one of them but code is not that said if i remember correctly the tribe clients shouldn't read from configuration file and sysprops but only inherit a few settings from the parent node like it happens in code something that we had enforced with i think something got lost with where code was removed which was our way to prevent loading anything from the config file with that set to code i believe we wouldn't even check for the existence of the file thus we wouldn't need any permission for that at this point it seems to me that we would have to forward code to the tribe clients just because we are going to check for its existence at some point although we have nothing to load from it otherwise we check for path home that we have no permissions for i think i'd need mention to verify if what i explained makes any sense it might be that i overlooked something
784,elasticsearch,14573,6,PC,if i understand the tribe node correctly it is no different than any other client node well creating multiple client nodes internally so to me it should be passing along any settings it needs to configure the node including code however i'm not sure what this has to do with the transport client the transport client by definition now does not use the config file settings and the stack trace shown above indicates the exception was from building a node not a transport client
785,elasticsearch,14573,7,[PC/SE],reference mention it doesn\'t have to do directly with the transport client but the inner tribe nodes have a similar requirement when it comes to loading from config file they should not be reading out of the config file but only inherit some selected settings from their parent node the actual tribe node and that is why we were previously setting code to code which is now removed though if my analysis is correct security manager barfs because we check if the config file exists while creating inner client nodes as part of code but we shouldn\'t need to read from that file at that point anyway i could forward the code setting to the client nodes too but i feel it is not the right fix given that we should not be reading from that file nor check if it exists not sure what the right fix is though 
786,elasticsearch,14573,8,PA,i looked deeper i can confirm this is not just a problem around passing in the right code to the inner nodes the inner client nodes must not read from the main configuration file something that was fixed in the option to not load from config settings for a node was though removed with i had expected code to fail after that change but it doesn't unfortunately if you try setting for instance code in the configuration file the tribe node will get that port but the inner nodes will try to get that one too and will fail the inner nodes should only get some selected settings from their parent node but never read from config file or system properties
787,elasticsearch,14573,9,PC, removing the path conf did not resolve the issue the config used codearea
788,elasticsearch,14573,10,SD,there is a workaround for this bug assuming your tribe config directory is code : codearea then edit code and specify a code for each tribe cluster eg: codearea then start elasticsearch as: codearea the tribe node will use code as its config directory then the tribe node starts a node client for each cluster and will use code as its config directory but code is empty so no settings will be loaded 
789,elasticsearch,14573,11,SE,workaround above works the only caveat is that depending on where the additional empty configuration file is located we might not have the permissions to read from it i think it should work if we simply add an empty configuration file under the tribe node config and point right to it not only specifying its parent directory but the complete path that includes the filename: codearea
790,elasticsearch,14573,12,RC,ran into this last night when attempting to set up a tribe node on this will also affect users who attempt to set a custom transport tcp port for the tribe node in this case setting a custom transport tcp port for the tribe node causes a misleading code exception when the port specified is not actually already in use codearea settings for the clusters: codearea and codearea the problem is that the tribe node will not start up as long as i have the transport tcp port: in place if i don't set a custom transport port for the tribe node it starts up fine and can connect with the clusters the following is the error that shows up when i attempt to set transport tcp port for the tribe node note that prior to starting the tribe node i used lsof to confirm that there's no process on the machine using port and it doesn't matter what port i set it to as long as transport tcp port is set for the tribe node it will throw the same bind exception codearea note that i cannot reproduce this on on i can set up a custom transport tcp port for the tribe node and it will start up fine
791,elasticsearch,14573,13,PA,mention this happens because the tribe node process will start three nodes the first one will get the configured port and the second will try to get the same one as it reads from the same configuration file the workaround provided by clint above should work till we fix this properly 
792,elasticsearch,14573,14,SD,mention i am going to explore having the tribe node have its own subclass of node which can customize this single behavior how to get the node's settings i don't think we should add back this general purpose flag as we need to keep the tons of ways nodes can be configured to a minimum
793,elasticsearch,14573,15,NT,mention thanks that sounds good to me 
794,elasticsearch,14573,16,SE,confirmed that the workaround works to prevent the bindtransportexception error thx 
795,elasticsearch,14573,17,PI,mention do we have a sense of whether the fix will make it to the upcoming release or will it likely be after i e use the workaround until a later x release 
796,elasticsearch,14573,18,PA,mention definitely after i would not want to destabilize with a refactoring like this 
797,elasticsearch,14573,19,NT,mention sounds good thx 
798,elasticsearch,14573,20,SD,this requires some fairly extensive changes so we will target this for in the meantime we should document the workaround in the docs 
799,elasticsearch,14573,21,SD,i opened a pr to fix this here: note that i was able to do the fix simply enough that i think it will be ok to backport to x
800,elasticsearch,14573,22,NT,thanks mention thanks mention
801,elasticsearch,14573,23,NT,thanks mention 
802,elasticsearch,14573,24,RC,i\'m late to the party but thought this might be useful for anyone coming across this i found that the dummy config file isn\'t needed to work around the issue instead for creating a new directory etc tribe client in the example path conf can reference the current configuration directory using the above example where the config directory was etc tribe arbitrary config transport tcp port: urlothers network host: path data: var lib elasticsearch path logs: var log elasticsearch tribe: kibana: path conf: etc tribe cluster name: logstash kibana discovery zen ping multicast enabled: false discovery zen ping unicast hosts: els: path conf: etc tribe cluster name: logstash data discovery zen ping multicast enabled: false discovery zen ping unicast hosts: 
803,elasticsearch,14573,25,RR,is this fixed in 
804,elasticsearch,14573,26,RO,with v i still have to specify path conf and i used the valid path as mentioned above by lb in my case i also had to specify path plugins for similar reason otherwise i kept getting accesscontrolexception error i did not have to specify both path conf and path plugins when i was using v 
805,elasticsearch,14573,27,RO,wrt es v i have to do the following to get the tribe node talking to two different clusters: cluster a and cluster b tribe node\'s configuration elasticsearch yml network host: transport tcp port: urlothers urlothers true tribe t cluster name: cluster a tribe t discovery zen ping unicast hosts: cluster a\'s master node tribe t discovery zen ping multicast enabled: false tribe t path conf: valid path to conf tribe t path plugins: valid path to plugin tribe t network bind host: tribe t network publish host: tribe node\'s ip tribe t transport tcp port: optional but different from tribe node port above repeat the same block but replace t to t for cluster b and fill in proper info related to cluster b but keep the tribe t network \\ the same with different tribe t transport tcp port value from t if specified 
806,elasticsearch,14573,28,[RR/PA],mention setting network and path settings for tribe nodes the t t here should not be necessary can you share your full elasticsearch yml for both the tribe node as well as cluster a and cluster b 
807,elasticsearch,14573,29,RC,mention i did not have to do network and path settings when i was using v it was a surprise to me when v kept giving me accesscontrolexception error message initially it pointed to the plugins location after i set it it complained about the config location if i did not do the network settings for t and t it was not able to connect to cluster a and or b this part is weird too again i did not have to do this in v all es instances are installed using rpm file not zip file my settings for tribe node is above with additional parameters cluster name discovery zen ping multicast enabled: false cluster a and b each has master node data nodes with the following parameters\' settings i don\'t have all information with me at the moment cluster name: cluster a or b network host: transport tcp port: urlothers master urlothers true master discovery zen ping multicast enabled: false discovery zen ping unicast hosts: master node\'s ip path conf: data es config path plugins: data es plugins path data: data es
813,scikit-learn,10773,0,SD,if it doesn\'t regularize then i agree it should be deprecated there are too many parameters to tune especially when you don\'t realise one doesn\'t work on mar : am andreas mueller notifications mention wrote: the docs state that min samples leaf reduces the size of the tree this reference
814,scikit-learn,10773,1,SE,hi can i give this a shot if deprecation is intended 
815,scikit-learn,10773,2,SE,i think mention right that the current option results in bad splits and poor regularisation yes let's deprecate it on march at : farahsaeed notifications mention wrote: hi can i give this a shot if deprecation is intended reference
816,scikit-learn,10773,3,SE,i think deprecation is a good step i\'d kind of like to introduce the correct parameter which is what i intended and which is what\'s implemented in r\'s ranger random forests and c but i\'m not sure how to best do that and if it\'s worth it i think it would be strictly better than code min samples split code 
817,scikit-learn,10773,4,SE,mention it regularizes but it doesn\'t reduce the size of the tree 
818,scikit-learn,10773,5,SE,gotcha
819,scikit-learn,10773,6,PI,mention are you still working on this if not i'd like to take a look at this :
820,scikit-learn,10773,7,RC,mention how would you mitigate this issue 
821,scikit-learn,10773,8,SE,deprecate parameter and remove it from the docs i think the other option would be to actually implement the thing that is implemented in other libraries that would change behavior in a backward incompatible way but only if code min samples split reference though that would be kind of interesting for mention because his paper uses the current implementation very explicitly and shows that it's not useful
822,scikit-learn,10773,9,SD,ffr the paper: urlothers according to the functional anova analysis this hyperparameter is the most important over datasets functional anova does not take into account defaults but further analysis shows that this hyperparameter should often kept left to default probably also interesting for mention as autosklearn tries to optimize this parameter and might waste resources with that updated wrong link
823,scikit-learn,10773,10,NT,mention you can try this any time good luck with your attempt 
824,scikit-learn,10773,11,PO,i will take a stab at this 
825,scikit-learn,10773,12,SD,is it also appropriate to add a note to docstrings for basedecisiontree and basegradientboosting currently those docstrings are just liner this is an abstract class please use derived classes 
826,scikit-learn,10773,13,SE,fixed in urlpull
827,scikit-learn,11313,0,SE,yeah i think that block should be in a code for j in rangen features code loop 
828,scikit-learn,11813,0,SE,we've seen something similar elsewhere recently yes i assume percentile idx needs to be clipped due to floating point errors in percentile weight cdf pr welcome
829,scikit-learn,11813,1,SD,an alternative might be to slightly increase code after calculating code 
830,scikit-learn,11813,2,SE,i assume this would be very hard to write a test for 
831,scikit-learn,11813,3,NT,trying to come up with tests failed 
832,scikit-learn,12373,0,RR,hi mention thanks for the report to make it easier for us to help you it'd be nice if you could provide a minimal reproducible example urlothers which would result in the error you're encountering
833,scikit-learn,12373,1,RO,mention thanks for your reply i tuned the parameters of branching factor threshold when i changed the branching factor from to there is no this kind of issue i will try to reproduce this issue again and extract the training example 
834,scikit-learn,12373,2,RO,it's a duplicate of urlissue a minimal example would definitely help
835,scikit-learn,12373,3,NT,please continue conversation at 
836,scikit-learn,12413,0,RO,i had the same issue a few days ago with a custom estimator thanks for the minimal example mention i had the same issue a few days ago with a custom estimator thanks for the minimal example mention
837,scikit-learn,12413,1,RO,duplicate of 
838,scikit-learn,12413,2,?,hii thank you for posting this but if we put n jobs it will take much more right because n jobs uses all the processors did you find found any way to solve this issue or do we need much more powerful pc
839,scikit-learn,12413,3,RR,you should not have such an error with the last version of scikit learn which version are you using 
840,scikit-learn,12413,4,RO,the scikit learn version is and python 
841,scikit-learn,12413,5,PC,this is solved in versions referencethis is solved in versions referencethis is solved in versions referencethis is solved in versions referencethis is solved in versions referencethis is solved in versions referencethis is solved in versions referencethis is solved in versions referencethis is solved in versions referencethis is solved in versions referencethis is solved in versions referencethis is solved in versions referencethis is solved in versions referencethis is solved in versions referencethis is solved in versions referencethis is solved in versions reference
842,scikit-learn,12413,6,[NT/PR],i really appreciate your help thank u i will check after updating if that solve the problem after checking both the version i am still getting error the error is following brokenprocesspool: a task has failed to un serialize please ensure that the arguments of the function are all picklable 
843,scikit-learn,12413,7,RC,i am still encountering this problem when using either scikit learn versions or 
844,scikit-learn,12413,8,RR,mention could you post a reproducible example of the error 
845,scikit-learn,12413,9,RO,here i am sharing my complete code: code i have checked for both scikit learn versions and 
846,scikit-learn,12413,10,RC,i also have this problem python and scikit learn i can skip the error by code 
847,scikit-learn,12413,11,PC,mention i tried code on windows and it did not work i changed to use it on ubuntu and it well done but it just uses cpu not gpu may be it does not support gpu for ann training urlothers
848,scikit-learn,12413,12,RC,i also have this problem on window even if i am running with cpus i am using: joblib scikit learn python 
849,scikit-learn,12413,13,RC,i also have this problem using: python scikit learn but code helps
850,scikit-learn,12413,14,DT,could you please open a new issue with a reproducible example the error seems to concern only windows and custom estimators implemented using keras whereas the original issue was concerning any platform and any custom estimator
851,scikit-learn,12413,15,[RC/PC],reference my problem is the same example explained in previous comments and yes i'm running a bit windows and using keras my lines of code are: codearea error: codearea i'm using: python scikit learn spyder
852,scikit-learn,12413,16,DT,thanks for the details mention what i meant is that the original issue described in the top comment urlissue is not the same as the one you describe although similar the original issue was solved and closed it would be better to open a new issue with your error related to windows keras and joblib 
853,scikit-learn,12413,17,NT,reference reference reference thanks mention i opened a new issue 
854,scikit-learn,12413,18,SD,using n jobs solved the error on windows python 
855,scikit-learn,12413,19,SD,you need to install the joblib package if that error still happens set n jobs none 
856,scikit-learn,12413,20,SE,reference i know but i wanted to fix the error with code
857,scikit-learn,12413,21,SD,import os os environ 'kmp duplicate lib ok' 'true' hope these work for you
858,scikit-learn,12413,22,SE,i get the same error with python and sklearn under windows none of the above worked for me can this issue be solved somehow 
859,scikit-learn,12523,0,NT,thanks for the report could you please open this issue in urlothers with a link here so it's fixed upstream and we will keep this issue open until it's resolved in the vendored version
860,scikit-learn,12523,1,PI,reported upstream in urlissue